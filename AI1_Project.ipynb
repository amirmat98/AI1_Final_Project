{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JumpyGridWorld Project\n",
    "\n",
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals.py\n",
    "\n",
    "# Declaring rewards and stepsPerEpisode as global variables\n",
    "rewards = []\n",
    "steps_episode = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jumpy_GW.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# JumpyGridWorld Project\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m## Introduction\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m## JumpyGW.py\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mJumpyGW\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Initializing\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, size, number_obstacles):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# JumpyGridWorld Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## JumpyGW.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class JumpyGW:\n",
    "    \n",
    "    # Initializing\n",
    "    def __init__(self, size, number_obstacles):\n",
    "        self.grid_size = size\n",
    "        self.grid = np.zeros( (size,size) )\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (size - 1, size - 1)\n",
    "        self.obstacles = set()\n",
    "        self.agent_path = []\n",
    "\n",
    "        for i in range(number_obstacles):\n",
    "            obs_pos = ( np.random.randint(size), np.random.randint(size))\n",
    "            self.obstacles.add(obs_pos)\n",
    "\n",
    "\n",
    "    # Reset the environemtn from scrach\n",
    "    def reset(self):\n",
    "        self.grid = np.zeros( (self.grid_size, self.grid_size) )\n",
    "        self.agent_path = []\n",
    "        return self.start\n",
    "\n",
    "    # Verify the validity of a position in the grid        \n",
    "    def position_validation(self, position):\n",
    "        if ( (0 <= position[0] < self.grid_size) and (0 <= position[1] < self.grid_size) ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Determine whether a given position constitutes an obstacle\n",
    "    def obstacle_check(self, position):\n",
    "        if (position in self.obstacles):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Determine whether a given position represents the goal state\n",
    "    def goal_check(self, position):\n",
    "        if (position == self.goal):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Action Effect Dictionary Definiation\n",
    "    Action_Effect = {\n",
    "        0: (-1, 0), # UP\n",
    "        1: (1, 0), # Down\n",
    "        2: (0, -1), # Left\n",
    "        3: (0, 1), #Right\n",
    "    }\n",
    "    \n",
    "    # Execute the designated action and provide the updated position and reward\n",
    "    def action_execuation(self, position, action):\n",
    "        if action in self.Action_Effect:\n",
    "            delta = self.Action_Effect[action]\n",
    "            new_positon = (position[0] + delta[0], position[1] + delta[1])\n",
    "        else:\n",
    "            jump_direction = np.random.choice([-1,1], size = 2)\n",
    "            new_positon = (position[0] + jump_direction[0], position[1] + jump_direction[1])\n",
    "\n",
    "        # update postion if valid and not an obstacle\n",
    "        new_positon = self.update_position(position, new_positon)\n",
    "\n",
    "        # update grid\n",
    "        self.update_grid(position, new_positon)\n",
    "\n",
    "        # calculate reward\n",
    "        reward = self.calculate_reward(new_positon)\n",
    "\n",
    "        # add the current position to the agent's path\n",
    "        self.agent_path.append(new_positon)\n",
    "\n",
    "        return new_positon, reward\n",
    "    \n",
    "\n",
    "    # define update position\n",
    "    def update_position(self, position, new_positon):\n",
    "        if self.position_validation(new_positon) and not self.obstacle_check(new_positon):\n",
    "            return new_positon\n",
    "        else:\n",
    "            return position\n",
    "    \n",
    "    # define update grid\n",
    "    def update_grid(self, position, new_positon):\n",
    "        self.grid[position] = 0\n",
    "        self.grid[new_positon] = 1\n",
    "    \n",
    "    # define calculation reward\n",
    "    def calculate_reward(self, position):\n",
    "        if self.goal_check(position):\n",
    "            return 10\n",
    "        else:\n",
    "            return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearning_Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class QLearning_Agent:\n",
    "    # A Q-table for storage. Q-values representing the expected rewards for each possible action in every state\n",
    "    def __init__(self, state_size, number_actions, rate_learning, factor_discount, rate_exploration):\n",
    "        \"\"\"\n",
    "        Construct a Q-Learning Agent.\n",
    "\n",
    "        Args:\n",
    "        - state_size (tuple): Size of the state space.\n",
    "        - number_actions (int): Count of potential actions.\n",
    "        - rate_learning (float): Rate at which the agent learns (alpha).\n",
    "        - factor_discount (float): Rate at which future rewards are discounted (gamma).\n",
    "        - rate_exploration (float): Probability of choosing a random action (epsilon).\n",
    "        \"\"\"\n",
    "        # Set up a table for Q-values with initial zero values\n",
    "        self.qtable = np.zeros(state_size + (number_actions,))\n",
    "        self.rate_learning = rate_learning\n",
    "        self.factor_discount = factor_discount\n",
    "        self.rate_exploration = rate_exploration\n",
    "        self.number_actions = number_actions\n",
    "        self.path_episode = [] # Record of the agent's trajectory in an episode\n",
    "\n",
    "\n",
    "    # Using the epsilon-greedy approach to implement action selection logic\n",
    "    def action_choose(self, state):\n",
    "        \"\"\"\n",
    "        Decide on an action based on the current state using an epsilon-greedy approach.\n",
    "\n",
    "        Args:\n",
    "        - state (tuple): The current state from which to choose an action.\n",
    "\n",
    "        Returns:\n",
    "        - (int): The selected action.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.rate_exploration:\n",
    "            return np.random.randint(self.number_actions)\n",
    "        else:\n",
    "            return np.argmax(self.qtable[state])\n",
    "        \n",
    "    \n",
    "    # Using the Q-learning update rule to set up Q-table update code\n",
    "    def qtable_update(self, state, action, reward, state_next):\n",
    "        \"\"\"\n",
    "        Update the Q-table with a new value based on the agent's experience.\n",
    "\n",
    "        Args:\n",
    "        - state (tuple): The state prior to taking the action.\n",
    "        - action (int): The action that was taken.\n",
    "        - reward (float): The reward received after taking the action.\n",
    "        - state_next (tuple): The state after the action was taken.\n",
    "        \"\"\"\n",
    "\n",
    "        q_value_current = self.qtable[state][action]\n",
    "        q_value_next_max = np.max(self.qtable[state_next])\n",
    "        q_value_updated = (1 - self.rate_learning) * q_value_current + \\\n",
    "                          self.rate_learning * (reward + self.factor_discount * q_value_next_max)\n",
    "        self.qtable[state][action] = q_value_updated\n",
    "\n",
    "    def episode_path_reset(self):\n",
    "        \"\"\"\n",
    "        Clear the record of the agent's path for a new episode.\n",
    "        \"\"\"\n",
    "        self.path_episode = []\n",
    "\n",
    "    def get_policy_optimal(self, obstacles):\n",
    "        \"\"\"\n",
    "        Create an optimal policy based on the highest Q-values in the Q-table.\n",
    "\n",
    "        Args:\n",
    "        - obstacles (np.array): A grid indicating where obstacles are present.\n",
    "\n",
    "        Returns:\n",
    "        - policy_optimal (np.array): The derived optimal policy grid.\n",
    "        \"\"\"\n",
    "        # Initialize the policy grid, defaulting to -1 for obstacles\n",
    "        policy_optimal = np.full(self.qtable.shape[:-1], -1, dtype=int) \n",
    "\n",
    "        for state in np.ndindex(policy_optimal.shape):\n",
    "            if not obstacles[state]:\n",
    "                q_value_max = np.max(self.qtable[state])\n",
    "                actions_best = np.where(self.qtable[state] == q_value_max)[0]\n",
    "                action_chosen = np.random.choice(actions_best)  # Select among the best actions\n",
    "                policy_optimal[state] = action_chosen\n",
    "\n",
    "        return policy_optimal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from Globals_Variables import rewards, steps_episode\n",
    "\n",
    "\n",
    "# Function to convert actions into visual symbols\n",
    "def map_action_to_symbol(action):\n",
    "    arrow_map = {\n",
    "        0: '↑',  # Up\n",
    "        1: '↓',  # Down\n",
    "        2: '←',  # Left\n",
    "        3: '→',  # Right\n",
    "        4: '⇆'   # Jump\n",
    "    }\n",
    "    # Fallback to '?' when the action is not found in the mapping\n",
    "    if action in arrow_map:\n",
    "        return arrow_map.get(action)\n",
    "    else:\n",
    "        return '?'\n",
    "\n",
    "\n",
    "# Function to plot the agent's path on the grid\n",
    "def draw_agent_route (size, agent_path, axis):\n",
    "    axis.clear()\n",
    "    axis.set_title(r'$\\bf{Agent\\'s\\ Trajectory\\ Path}$', fontsize=14)\n",
    "    axis.set_xlabel('Grid Width')\n",
    "    axis.set_ylabel('Grid Height')\n",
    "    axis.set_xticks(np.arange(size+1))  # Add +1 to include the edge for better visualization\n",
    "    axis.set_yticks(np.arange(size+1))  # Add +1 to include the edge for better visualization\n",
    "    axis.grid(which='both', color='red', linestyle='-', linewidth=2)\n",
    "    x_coords = [coord[0] for coord in agent_path]\n",
    "    y_coords = [coord[1] for coord in agent_path]\n",
    "    axis.plot(np.array(y_coords)+0.5, np.array(x_coords)+0.5, marker='o', linestyle='-', color='blue')  # Add +0.5 to center the markers in the cells\n",
    "    axis.invert_yaxis()  # Invert y-axis to match the visualization of the grade\n",
    "\n",
    "\n",
    "# Function to create a heatmap representation of the highest Q-values\n",
    "def render_qvalue_heatmap(q_values, axis):\n",
    "    axis.clear()\n",
    "    optimal_qvalues = np.max(q_values, axis=2)\n",
    "    heatmap = axis.matshow(optimal_qvalues, cmap='cool')\n",
    "    colorbar = plt.colorbar(heatmap, ax=axis)\n",
    "    colorbar.ax.set_ylabel('Q-values', rotation=-90, va=\"bottom\")\n",
    "    axis.set_title('Heatmap of Optimal Q-values', pad=20)\n",
    "    axis.set_xlabel('Width of Grid')\n",
    "    axis.set_ylabel('Height of Grid')\n",
    "    axis.set_xticks(np.arange(q_values.shape[1]))\n",
    "    axis.set_yticks(np.arange(q_values.shape[0]))\n",
    "\n",
    "\n",
    "# Function to display the agent's policy on a grid\n",
    "def display_agent_policy_on_grid(q_values, axis):\n",
    "    axis.clear()\n",
    "    # Determine the best action (highest Q value) for each state\n",
    "    optimal_actions = np.argmax(q_values, axis=2)\n",
    "\n",
    "    action_values_mapping = {\n",
    "        0: 0.25,  # Assign 0.25 to 'Up' action\n",
    "        1: 0.50,  # Assign 0.50 to 'Down' action\n",
    "        2: 0.75,  # Assign 0.75 to 'Left' action\n",
    "        3: 1.0,   # Assign 1.0 to 'Right' action\n",
    "        4: 0.0    # Assign 0.0 to 'Jump' action\n",
    "    }   \n",
    "\n",
    "    # Map the optimal actions to their corresponding visualization values\n",
    "    policy_display_values = np.vectorize(action_values_mapping.get, otypes=[float])(optimal_actions)\n",
    "\n",
    "    # Create a matrix display of the policy values with a color map\n",
    "    color_axis = axis.matshow(policy_display_values, cmap='cool', aspect='equal')\n",
    "\n",
    "    # Configure the color map and normalization for the color bar\n",
    "    color_map = plt.cm.cool\n",
    "    color_norm = Normalize(vmin=0.0, vmax=1.0)\n",
    "\n",
    "    # Create a ScalarMappable for the color bar\n",
    "    scalar_mapper = ScalarMappable(cmap=color_map, norm=color_norm)\n",
    "    scalar_mapper.set_array([])\n",
    "\n",
    "    # Add a color bar with specific tick labels for each action\n",
    "    color_bar = plt.colorbar(scalar_mapper, ax=axis, ticks=[0.0, 0.25, 0.50, 0.75, 1.0])\n",
    "    color_bar.ax.set_yticklabels(['Jump', 'Up', 'Down', 'Left', 'Right'])  # Label the ticks with action names\n",
    "\n",
    "    # Overlay action arrows on the grid\n",
    "    for row_index, row in enumerate(optimal_actions):\n",
    "        for col_index, action_code in enumerate(row):\n",
    "            # action_arrow = action_values_mapping(action_code)\n",
    "            action_arrow = action_values_mapping[action_code]\n",
    "            axis.text(col_index, row_index, action_arrow, horizontalalignment='center', verticalalignment='center', fontsize = 10, color='black')\n",
    "\n",
    "    # Set grid lines centered on each cell edge\n",
    "    axis.set_xticks(np.arange(optimal_actions.shape[1]) - 0.5, minor=True)\n",
    "    axis.set_yticks(np.arange(optimal_actions.shape[0]) - 0.5, minor=True)\n",
    "    \n",
    "    # Draw the grid lines\n",
    "    axis.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
    "    \n",
    "    # Remove tick marks\n",
    "    axis.tick_params(axis='both', which='both', length=0)\n",
    "    \n",
    "    # Set the title for the axes\n",
    "    axis.set_title('Optimal Policy Visualization\\n')\n",
    "\n",
    "\n",
    "# Function to update visualization plots after training episodes\n",
    "def refresh_visualization(episode_number, agent, axis_steps, axis_rewards, axis_cumulative_rewards, axis_policy, axis_qvalues, axis_path, environment, path_of_episode):\n",
    "    global rewards, steps_episode  # Referencing global variables\n",
    "    final_episode = agent.number_episodes - 1  # Check if it's the last episode\n",
    "    if episode_number == final_episode:  # Update plots only after the last training episode\n",
    "        render_qvalue_heatmap(agent.qtable, axis_qvalues)\n",
    "        display_agent_policy_on_grid(agent.qtable, axis_policy)\n",
    "        axis_policy.set_title(r'$\\bf{Visualization\\ of\\ Policy\\ Grades}$', fontsize=14)\n",
    "        axis_qvalues.set_title(r'$\\bf{Heatmap\\ of\\ Q-values}$', fontsize=14)\n",
    "        axis_path.set_title(r'$\\bf{Path\\ Taken\\ by\\ Agent}$', fontsize=14)\n",
    "\n",
    "        # Plot the number of steps per episode\n",
    "        axis_steps.plot(range(episode_number + 1), steps_episode, color='blue')\n",
    "        axis_steps.set_xlabel('Episode')\n",
    "        axis_steps.set_ylabel('Steps')\n",
    "        axis_steps.set_title(r'$\\bf{Episode\\ vs.\\ Steps}$', fontsize=14)\n",
    "\n",
    "        # Plot the rewards obtained per episode\n",
    "        axis_rewards.plot(range(episode_number + 1), rewards, color='magenta')\n",
    "        axis_rewards.set_xlabel('Episode')\n",
    "        axis_rewards.set_ylabel('Reward')\n",
    "        axis_rewards.set_title(r'$\\bf{Episode\\ vs.\\ Rewards}$', fontsize=14)\n",
    "\n",
    "        # Plot the accumulated rewards over all episodes\n",
    "        accumulated_rewards = np.cumsum(rewards)\n",
    "        axis_cumulative_rewards.plot(range(episode_number + 1), accumulated_rewards, color='green')\n",
    "        axis_cumulative_rewards.set_xlabel('Episode')\n",
    "        axis_cumulative_rewards.set_ylabel('Total Reward')\n",
    "        axis_cumulative_rewards.set_title(r'$\\bf{Episode\\ vs.\\ Cumulative\\ Reward}$', fontsize=14)\n",
    "\n",
    "        # Display the agent's path during the episode\n",
    "        draw_agent_route(environment.grid_size, path_of_episode, axis_path)\n",
    "\n",
    "        # Adjust the spacing between plot rows and columns\n",
    "        plt.subplots_adjust(hspace=0.4, wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Jumpy_GW import JumpyGW\n",
    "from QLearning_Agent import QLearning_Agent\n",
    "from Plotter import draw_agent_route, render_qvalue_heatmap, display_agent_policy_on_grid, refresh_visualization\n",
    "from Globals_Variables import rewards, steps_episode\n",
    "from Plotter import map_action_to_symbol\n",
    "\n",
    "\n",
    "# Function to set up the simulation with a predefined environment and agent\n",
    "def configure_simulation():\n",
    "    # Seed the random number generator for reproducibility\n",
    "    # temp_seed = np.random.randint(0, 100)\n",
    "    # print(\"seed = \", temp_seed)\n",
    "    # np.random.seed(temp_seed)\n",
    "    np.random.seed(56)\n",
    "\n",
    "    # Define parameters for the grid world and Q-learning agent\n",
    "    params = {\n",
    "        'grid_size': 10,         # Size of the grid world\n",
    "        'number_obstacles': 5,        # Number of obstacles in the grid world\n",
    "        'state_size': (10, 10), # Shape of the state space\n",
    "        'number_actions': 5,          # Number of possible actions\n",
    "        'learning_rate': 0.1,    # Learning rate for Q-learning updates\n",
    "        'discount_factor': 0.9,  # Discount factor for future rewards\n",
    "        'exploration_rate': 0.1, # Rate at which the agent explores the environment\n",
    "        'episodes': 1000         # Number of episodes to run the simulation\n",
    "    }\n",
    "\n",
    "    # Initialize the grid world and agent with the specified parameters\n",
    "    environment = JumpyGW(params['grid_size'], params['number_obstacles'])\n",
    "    agent = QLearning_Agent(params['state_size'], params['number_actions'], params['learning_rate'], params['discount_factor'], params['exploration_rate'])\n",
    "    agent.number_episodes = params['episodes']\n",
    "    \n",
    "    return environment, agent, params\n",
    "\n",
    "# Function to run the simulation of the environment and agent interaction\n",
    "def run_simulation(environment, agent, params):\n",
    "    # Set up plotting\n",
    "    figure, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes_list = axes.flatten()\n",
    "    # ax1, ax2, ax3, ax4, ax5, ax6 = axes.flatten()\n",
    "\n",
    "    # Clear any previous metrics\n",
    "    rewards.clear()\n",
    "    steps_episode.clear()\n",
    "    grid_layouts = []\n",
    "    performance = []\n",
    "    episode_path = []\n",
    "\n",
    "    # Run the simulation for the specified number of episodes\n",
    "    print(\"Ongoing training...\")\n",
    "    for episode_num in range(params['episodes']):\n",
    "        current_state = environment.reset() # Reset the environment\n",
    "        total_reward = 0 # Initialize total reward for the episode\n",
    "        episode_qvalues = []\n",
    "        \n",
    "        agent.episode_path_reset() # Initialize agent's path for the new episode\n",
    "\n",
    "        # Iterate until the agent reaches a final state\n",
    "        while not environment.goal_check(current_state):\n",
    "            action = agent.action_choose(current_state) # Agent selects an action\n",
    "            next_state, reward = environment.action_execuation(current_state, action) # Environment responds\n",
    "            agent.qtable_update(current_state, action, reward, next_state) # Agent updates its knowledge\n",
    "\n",
    "            current_state = next_state # Move to the next state\n",
    "            total_reward += reward # Accumulate rewards\n",
    "            episode_qvalues.append(np.copy(agent.qtable[current_state]))\n",
    "            agent.path_episode.append(current_state) # Record the new state\n",
    "\n",
    "        # Record episode metrics\n",
    "        rewards.append(total_reward)\n",
    "        steps_taken = len(episode_qvalues)\n",
    "        steps_episode.append(steps_taken)\n",
    "        performance.append(total_reward / steps_taken)\n",
    "\n",
    "        # Plot statistics after each episode\n",
    "        grid_layout = np.zeros_like(environment.grid)\n",
    "        obstacle_position = list(environment.obstacles)\n",
    "        grid_layout[tuple(zip(*obstacle_position))] = -1\n",
    "        grid_layout[environment.goal] = 0.5\n",
    "        grid_layout[current_state] = 0.8\n",
    "        grid_layouts.append(grid_layout)\n",
    "\n",
    "        # Update episode_path\n",
    "        episode_path = agent.path_episode              \n",
    "        refresh_visualization(episode_num, agent, axes_list[0], axes_list[1], axes_list[2], axes_list[3], axes_list[4], axes_list[5], environment, episode_path)\n",
    "\n",
    "        # Visualize the agent's trajectory path\n",
    "        draw_agent_route(params['grid_size'], episode_path, axes_list[5])\n",
    "\n",
    "    # Simulation is complete\n",
    "    print(\"Training has been completed.\")\n",
    "\n",
    "    # Extract and display the optimal policy derived from Q-values\n",
    "    optimal_policy = make_optimal_policy(environment, grid_layouts, agent)\n",
    "    display_policy(optimal_policy)\n",
    "    plt.show() # Display all the plots\n",
    "\n",
    "# Function to make the optimal policy\n",
    "def make_optimal_policy(env, grid_layouts, agent):\n",
    "    optimal_policy = np.zeros_like(env.grid, dtype = int)\n",
    "    for i in range(optimal_policy.shape[0]):\n",
    "        for j in range(optimal_policy.shape[1]):\n",
    "            if grid_layouts[-1][i, j] != -1:\n",
    "                optimal_policy[i, j] = np.argmax(agent.qtable[i, j])\n",
    "    \n",
    "    return optimal_policy\n",
    "\n",
    "# \n",
    "# Function to display the optimal policy in a human-readable format\n",
    "def display_policy(policy):\n",
    "    print(\"\\nOptimal Solution and Policy:\")\n",
    "    # Iterate through all states in the policy\n",
    "    for i in range(policy.shape[0]):\n",
    "        for j in range(policy.shape[1]):\n",
    "            action = policy[i, j]\n",
    "            print(f\"State ({i}, {j}): Move {map_action_to_symbol(action)}\")\n",
    "\n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    print(policy)\n",
    "\n",
    "# Main entry point for the script\n",
    "def main():\n",
    "    # Configure the environment and agent, and fetch simulation parameters\n",
    "    environment, q_agent, simulation_params = configure_simulation()\n",
    "    # Run the simulation with the configured environment and agent\n",
    "    run_simulation(environment, q_agent, simulation_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() # Execute the main function if the script is run directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
